---
  title: "Forecasting modeling of categorical target Assignment"
author: "Katerina Dimitrova, Jose Romero, Sergi Munoz"
date: "5 June, 2018"
output: pdf_document
editor_options: 
  chunk_output_type: console
---
  
  ```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

```{r,include=FALSE}
#setwd("C://Users/Sergi/Desktop/Sergi/ADEI") #Change 
setwd("D:/ADEI/ADEI.git/trunk") #Change

```

#Previous work

## Load requiered packages

```{r, echo=FALSE, include=FALSE}
rm(list=ls())
# Load Required Packages: to be increased over the course

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","missMDA","mvoutlier","dplyr","caret", "ggmap","ggthemes","knitr","MVA", "ROCR")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

# Useful function
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3],        q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

```

# Statistical Modelling

## Load your sample after data cleaning and validation (Deliverable 1)


```{r}
load("Taxi5000_raw_DataDefinitivev1.RData")
names(df)
#summary(df)
#vars_con
#vars_dis
#vars_res


```
#Split data 70-30
```{r}
llwork<-sample(1:nrow(df),0.70*nrow(df),replace=FALSE)
llwork<-sort(llwork);length(llwork)
dfwork<-df[llwork,]
dftest<-df[-llwork,]
```

#Best model is using  Pickup_latitude + Dropoff_longitude + Trip_distance + MTA_tax? + Tolls_amount?, ? make sense? 
```{r}

names(dfwork)
catdes(df,num.var=22)
vars_cexp<-vars_con[c(1:4,5,6,7,8,9,11,13)]# 10 and 12 do problems?
dfX <- dfwork[,c("AnyTip",vars_cexp)]
cor(dfwork[,c(vars_cexp)])
m50<-glm(AnyTip~.,family="binomial",data=dfX)
summary(m50)
Anova(m50,test="Wald")
vif(m50)

m51<-step(m50,k=log(nrow(dfwork)))
summary(m51)
Anova(m51,test="Wald")
vif(m51)

m52<-glm(AnyTip~ (Pickup_latitude + Dropoff_longitude + Trip_distance)*claHP ,family="binomial",data=dfwork)
summary(m52)
#exageratted colinearity?
vif(m52)

#it gets rid of any interaction
m62<-step(m52,k=log(nrow(dfwork)))
summary(m62)


m53<-glm(AnyTip~ (Pickup_latitude + Dropoff_longitude)*claHP ,family="binomial",data=dfwork)
summary(m53)
vif(m53)

#Dropoff_longitude + claHP
m63<-step(m53,k=log(nrow(dfwork)))
summary(m63)
vif(m63)

#not the same, so we compare and choose m63
anova(m53,m63, test = "Chisq")
BIC(m63,m53)

#too complexity
m54<-glm(AnyTip~ (Pickup_latitude + Dropoff_longitude)+(claHP*f.distance) ,family="binomial",data=dfwork)
summary(m54)
#too much colinearity claHP
vif(m54)

m64<-step(m54,k=log(nrow(dfwork)))
#Dropoff_longitude + claHP + f.distance
summary(m64)
#we would choose m64
BIC(m63, m64)

#but colinearity test points that m64 has some while m63 is ok
vif(m63)
vif(m64)

m55<-glm(AnyTip~ Pickup_latitude + claHP + f.distance + Pickup_latitude:claHP,family="binomial",data=dfwork)
summary(m55)

m65<-step(m55,k=log(nrow(dfwork)))
summary(m65)

vif(m55)
vif(m65)

m56<-glm(AnyTip~ Pickup_latitude + Dropoff_longitude * f.distance, family="binomial",data=dfwork)
summary(m56)
vif(m56)

m66<-step(m56,k=log(nrow(dfwork)))
summary(m66)
vif(m66)

m57<-glm(AnyTip~ Dropoff_longitude * (f.distance+claHP), family="binomial",data=dfwork)
summary(m57)
vif(m57)

m67<-step(m57,k=log(nrow(dfwork)))
summary(m67)
vif(m67)


BIC(m55,m56)
BIC(m63,m64)
BIC(m63,m65)
BIC(m64,m66)
BIC(m64,m67)

summary(m67)

m58<-glm(AnyTip~ Dropoff_longitude:claHP + f.distance, family="binomial",data=dfwork)
summary(m58)
vif(m58)

BIC(m64,m58)
```
# Diagnostics
```{r}
#Residuals
Boxplot(cooks.distance(m51))

llcoo<-which(cooks.distance(m51)>0.01);length(llcoo)
llista<-influencePlot(m51,id.n=8)
influencePlot(m51,id.n=5)
attributes(llista)
influenceIndexPlot(m51)
dfwork[llcoo,]
#Remove influential data
llfora1<-row.names(llista);llfora1;length(llfora1)
ll<-which(row.names(dfwork)%in%llfora1);ll;length(ll)
df1<-dffwork[-ll,]

plot(allEffects(m51))
p51<-predict(m51,type="response")
p51<-predict(m51)



fit.AnyTip<-factor(ifelse(predict(m51,type="response")<0.5,0,1),labels=c("fit.No","fit.Yes"))
tt<-table(fit.AnyTip,dfwork$AnyTip);tt
sum(tt)
100*sum(diag(tt))/sum(tt)


#Predict using test data partition
p <- predict(m51, newdata=subset(dftest, type="response"))
pr <- prediction(p, dftest$AnyTip)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
table(dftest$AnyTip, (p > 0.5))
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc #Accuracy
par(mfrow=c(1,2))
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)
#---------------------------------------------------------------------------------------------

#Here we calculate how would we predict without any model
m0<-glm(AnyTip~1, family="binomial", data=dfwork)
fit0<-predict(m0,type="response")
fit.AnyTip0<-factor(ifelse(fit0<0.5,0,1),labels=c("fit.Yes"))

tt0<-table(fit.AnyTip0,dfwork$AnyTip);tt0;sum(tt0)
sum(tt0[1,2])/sum(tt0) #Accuracy without the prediction model

#Repeating test, i dont know why.....
dadesroc<-prediction(predict(m51,type="response"),dfwork$AnyTip)
par(mfrow=c(1,2))
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)

residualPlots(m51) # USeful?, I think not but I wont remove it since I am not sure?

```


