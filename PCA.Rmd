---
title: "PCA analysis of NYCABS datase"
author: "Katerina Dimitrova, Jose Romero, Sergi Munoz"
date: "March 18, 2018"
output: pdf_document
---

```{r,include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

```
```{r,include=FALSE}
setwd("C://Users/Sergi/Desktop/Sergi/ADEI") #Change 


```

#Loading function
```{r}

calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

```

#I Introduction
1. The Kaiser rule is to drop all components with eigenvalues under 1.0 
   According to the Elbow rule when the drop ceases and the curve makes an elbow toward less steep declinewe should drop all further components after the one starting the elbow.
#JAJA
```{r}
load("Taxi5000_raw_libraryDay.RData")
library(FactoMineR)
names (df)
vars_con<-names(df)[c(6,7,8,9,10,11,12,13,14,15,16,17,18,22,23,24,25,26)] 
vars_con
#actives<-names(df[c(10,12,15,23,24,25,28,18)])# only quantative
##qualsup<-names(df[c(22)])

#quantsup<-names(df[c(18)])
vars_dis

#WHAT SHE HAS DONE IN CLASS (IT IS NOT OUR VECTORS SO IT WON'T WORK)
# f.hour it is not a good choice anyway, she prefers period (conclusions made in clusters analysis)
# we have to choose suplementary variable that would help us to interpret -> it's a good idea to use discretees homologous of the variables that contribute the most for pca projection.
#res.pca<-PCA(df[,c(vars_conpca, "f.fare", "f.tt", "f.hour")], ncp=4, quanti.sup=which(vars_conpca=="Total_amount"), ind.sup = llvout, quali.sup = 19:21, graph= FALSE)

res.pca<-PCA(df[,vars_con], quanti.sup = 13, quali.sup = 14, ncp = 4 ) # TotalAmount and AnyTip
barplot(res.pca$eig[,1], main="Eigenvalues", names.arg = paste("dim", 1:nrow(res.pca$eig)))
length <-length(which(res.pca$eig[,1]>=1))
#kaiser <- res.pca$eig[1:length,1] #keep only EV >=1 ->first 6
#length(res.pca$ind[,1])
#kaiser # first 6 
elbow <- res.pca$ind$coord #above the elbow first 5 dimentions

plot(res.pca$eig[,1], type = "l") # line chart

#facto extra
fviz_eig(res.pca, addlabels = TRUE)
fviz_eig(res.pca, choice = "eigenvalue",addlabels = TRUE)

#look at variables that are too contributive
names (res.pca$ind)
summary(res.pca, dig = 2, nbelements = 17, nbind=3, ncp=4)

#numeric, EDA(?)
res.pca$ind$coord[,1] #coord of all ind. for 1rst dimension
ll1<-Boxplot(res.pca$ind$coord[,1])
calcQ(res.pca$ind$coord[,1]) # doesn't work 'cause we have to load the function
rang1<-sort(res.pca$ind$coord[,1],decreasing = T)
df[rang1[1:3], vars_con]

#graphic
plot.PCA(res.pca, choix=c("ind"),cex=0.8,col.ind="grey80",select="contrib15",axes=c(1,2))



#characteristic of extreme otliers in dim1 or sth like this
summary(res.pca$ind$coord[,1])
iqrvar<-IQR(res.pca$ind$coord[,1])
quantil3<-quantile(res.pca$ind$coord[,1], .75);quantil3 #get 3rd quartile
outliers<-which(res.pca$ind$coord[,1]>(iqrvar*3)+quantil3);length(outliers)


df$f.outlierPCAd1<-outliers
names(df)
#catdes(,names(df)[c(22)])


rang1<-sort(res.pca$ind$coord[,1],decreasing = T)
#rang1[1:3]
#df[rang1[1:3],vars_con]
df$epca1<-0
df$epca1[rang1[1:length(outliers)]]<-1

summary(df$epca1)
df$epca1<-factor(df$epca1,labels=c("NoOutDim1", "YesOutDim1"))
names(df)
catdes(df, 42)

# Use supplementary individuals



#III Interpret axis

# Interential criteria
dimdesc (res.pca, axes=1:4)

plot(res.pca,choix="var", cex = 0.75)
plot(res.pca,choix="var", cex = 0.75, axes = (3:4))# 3rd and 4th PCA
#modern factoextra

fviz_pca_var(res.pca,col.var="cos2", repel=TRUE)+scale_color_gradient2(low="green", mid="blue", high="red",midpoint=0.5)+theme_bw() #Almost

###last section missing
#lines(res.pca)#...


```
#IV K-Means Classification

```{r}
#partitioning in k=7 groups
res.pca<- PCA(df[,c(vars_con)], ncp=4,quanti.sup=which(vars_con=="Total_amount"),ind.sup = llvout, graph = FALSE)

ppcc<-res.pca$ind$coord[,1:4]
dim(ppcc)
kc<-kmeans(ppcc,6,iter.max = 30, trace=T)

df$claKM<-7
df[names(kc$cluster),"claKM"]<-kc$cluster
kc$betweenss/kc$totss

#caracterització claKM
catdes(df, 39)
#veure si s'han posat d'acord o no
table(df$claHP, df$claKM)

#assignar 
df$claHp<-factor(df$claHP,labels=paste("kHp-",1:7))
df$claKM<-factor(df$claKM, levels=c(6,5,7,1,2,4,3,8), labels = ...)

#-----------------------------------------------#
kMeansCluster <- kmeans(elbow, 4, nstart = 20)

kMeansCluster <- kmeans(res.pca$ind$coord[,1:6],center= 3)


kMeansCluster$clusterF <-as.factor(kMeansCluster$cluster)
elbowDF<-data.frame(elbow)
ggplot(elbowDF, aes(elbowDF$Dim.1,elbowDF$Dim.2 , color = kMeansCluster$clusterF)) + geom_point()
```

#V Hierarchical clustering

```{r}
library(FactoMineR)
library (ggplot2)
library(ggdendro)

summary(df[,vars_con])
#we need to do at least 6-7-8 clusters
res.hcpc <-HCPC(res.pca)
names(res.hcpc)
table (res.hcpc$data.clust$clust)
#Block A descripció per variables
res.hcpc$desc.var
#Block B descripció per eixos
#She doesn't recommend that (not very usefull by her opinion)
res.hcpc$desc.axes
#Block C individus
#parangons. per cadascun dels clusters tenim els seus parangons i els seus més espeicifis (dist) -> que es diferencien més dels altres clusters
res.hcpc$desc.ind

#Donar-li una classe (la última) a tots els outliers multidimensionals (sup.)

#No ens hem de preocupar de la clase outliers, només caracteritizar els clusters del metode per defecte que genera la classificació jerarquica. 

#clusters <- hclust(dist(elbowDF), method = 'mcquitty')
#ggdendrogram(clusters, rotate = FALSE, size = 2)
#plot(clusters)
#clusterCut <- cutree(clusters, 3)
#ggplot(elbowDF, aes(elbowDF$Dim.1,elbowDF$Dim.2)) + 
 # geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + A
  #scale_color_manual(values = c('yellow', 'red', 'green'))

```