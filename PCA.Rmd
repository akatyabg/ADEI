---
title: "PCA analysis of NYCABS datase"
author: "Katerina Dimitrova, Jose Romero, Sergi Munoz"
date: "March 18, 2018"
output: pdf_document
---

```{r,include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

```

```{r,include=FALSE}
#setwd("C://Users/Sergi/Desktop/Sergi/ADEI") #Change 
setwd("D:/ADEI/ADEI.git/trunk") #Change

```

#Previous work

## Load requiered packages

```{r,include=FALSE}
rm(list=ls())
requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","missMDA","mvoutlier","dplyr","ggmap","ggthemes","knitr","MVA")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

# Useful function
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3],        q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

```


#PCA analysis
1. The Kaiser rule is to drop all components with eigenvalues under 1.0 
   According to the Elbow rule when the drop ceases and the curve makes an elbow toward less steep declinewe should drop all further components after the one starting the elbow.
   
##I. I. Eigenvalues and  axes 
```{r}
load("Taxi5000_raw_DataClean.RData")
library(FactoMineR)
names (df)
vars_con_pca<-c(6,7,8,9,10,11,12,13,14,15,16,17,18,22,23,24,25,26) 

#vars_con_pca<-names(df)[c(6,7,8,9,10,11,12,13,14,15,16,17,18,22,23,24,25,26)] 
res.pca<-PCA(df[,vars_con_pca], quanti.sup = 13, quali.sup = 14, ncp = 6 ) # TotalAmount and AnyTip
barplot(res.pca$eig[,1], main="Eigenvalues", names.arg = paste("dim", 1:nrow(res.pca$eig)))
plot(res.pca$eig[,1], type = "l") # line chart
length <-length(which(res.pca$eig[,1]>=1));length
kaiser <- res.pca$eig[1:length,1] #keep only EV >=1 ->first 6

#facto extra
fviz_eig(res.pca, addlabels = TRUE)
fviz_eig(res.pca, choice = "eigenvalue",addlabels = TRUE)
elbow <- kaiser ## we also stop at 6th dimension because of the graphic (the slope stops at 6th dimension)

```

##II.  Individuals point of view
## Look at variables that are too contributive
```{r}
summary(res.pca, dig = 2, nbelements = 17, nbind=3, ncp=4)

#ll1<-Boxplot(res.pca$ind$coord[,1]);ll1 #outliers from coord of all ind. for 1rst dimension
#rang1<-sort(res.pca$ind$coord[,1],decreasing = T)
#df[rang1[1:3], vars_con]

#graphic
plot.PCA(res.pca, choix=c("ind"),cex=0.8,col.ind="grey80",select="contrib15",axes=c(1,2))

#DIMENSION1
#characteristic of extreme otliers in dim1
summary(res.pca$ind$coord[,1])
iqrvar<-IQR(res.pca$ind$coord[,1])
quantil3<-quantile(res.pca$ind$coord[,1], .75);quantil3 #get 3rd quartile
outliers<-which(res.pca$ind$coord[,1]>(iqrvar*3)+quantil3);length(outliers)

df$f.outlierPCAd1<-0
df[outliers,"f.outlierPCAd1"]<-1
df$f.outlierPCAd1<-factor(df$f.outlierPCAd1,labels=c("NoOutDim1", "YesOutDim1"))
summary(df$f.outlierPCAd1)
names(df)
#catdes(,names(df)[c(22)])

#DIMENSION2
#characteristic of extreme otliers in dim1
summary(res.pca$ind$coord[,2])
iqrvar<-IQR(res.pca$ind$coord[,2])
quantil3<-quantile(res.pca$ind$coord[,2], .75);quantil3 #get 3rd quartile
outliers2<-which(res.pca$ind$coord[,2]>(iqrvar*3)+quantil3);length(outliers2)


df$f.outlierPCAd2<-0
df[outliers2,"f.outlierPCAd2"]<-1
df$f.outlierPCAd2<-factor(df$f.outlierPCAd2,labels=c("NoOutDim2", "YesOutDim2"))
summary(df$f.outlierPCAd2)
names(df)

#DIMENSION3
#characteristic of extreme otliers in dim1
summary(res.pca$ind$coord[,3])
iqrvar<-IQR(res.pca$ind$coord[,3])
quantil3<-quantile(res.pca$ind$coord[,3], .75);quantil3 #get 3rd quartile
outliers3<-which(res.pca$ind$coord[,3]>(iqrvar*3)+quantil3);length(outliers3)


df$f.outlierPCAd3<-0
df$f.outlierPCAd3<-factor(df$f.outlierPCAd3,labels=c("NoOutDim3"))
summary(df$f.outlierPCAd3)
names(df)

#DIMENSION4
#characteristic of extreme otliers in dim1
summary(res.pca$ind$coord[,4])
iqrvar<-IQR(res.pca$ind$coord[,4])
quantil3<-quantile(res.pca$ind$coord[,4], .75);quantil3 #get 3rd quartile
outliers4<-which(res.pca$ind$coord[,4]>(iqrvar*3)+quantil3);length(outliers4)


df$f.outlierPCAd4<-0
df$f.outlierPCAd4<-factor(df$f.outlierPCAd4,labels=c("NoOutDim4"))
summary(df$f.outlierPCAd4)
names(df)

llvout<- c(outliers, outliers2);length(llvout)

catdes(df, 43)

```


##III Interpret axis
```{r}
# Interential criteria
dimdesc (res.pca, axes=1:4)

plot(res.pca,choix="var", cex = 0.75)
plot(res.pca,choix="var", cex = 0.75, axes = (3:4))# 3rd and 4th PCA
#modern factoextra

fviz_pca_var(res.pca,col.var="cos2", repel=TRUE)+scale_color_gradient2(low="green", mid="blue", high="red",midpoint=0.5)+theme_bw() #Almost

###last section missing
#lines(res.pca)#...


```

##IV PCA execution with supplementary individuals
```{r}
vec_out <- llvout
vars_con_pca<-names(df)[c(6,7,8,9,10,11,12,13,14,15,16,17,18,22,23,24,25,26)] 

vars_con_pca <- names(df)[c(6:16,18,23:26)]
#WHAT SHE HAS DONE IN CLASS (IT IS NOT OUR VECTORS SO IT WON'T WORK)
# f.hour it is not a good choice anyway, she prefers period (conclusions made in clusters analysis)
# we have to choose suplementary variable that would help us to interpret -> it's a good idea to use discretees homologous of the variables that contribute the most for pca projection.
res.pca<-PCA(df[,c(vars_con_pca, "f.fare_amount", "f.total", "pick_up_period")], ncp=6, quanti.sup=which(vars_con_pca=="Total_amount"), ind.sup = vec_out, quali.sup = 17:19, graph= FALSE) ##still not working
plot(res.pca,choix="var", cex = 0.75)
plot(res.pca,choix="var", cex = 0.75, axes = (3:4))# 3rd and 4th PCA
fviz_pca_var(res.pca,col.var="cos2", repel=TRUE)+scale_color_gradient2(low="green", mid="blue", high="red",midpoint=0.5)+theme_bw() #Almost


```


#Hierarchical clustering

Generem 8 clusters amb el mètode Hierarchichal a partir de les projeccions obtingudes amb el PCA.

```{r}
library(FactoMineR)
library (ggplot2)
library(ggdendro)

res.hcpc <-HCPC(res.pca, nb.clust = 6, order=TRUE)
table (res.hcpc$data.clust$clust)
```


```{r}

#Block A descripció per variables
res.hcpc$desc.var
```

```{r}

#Block B descripció per eixos
#She doesn't recommend that (not very usefull by her opinion)
res.hcpc$desc.axes
```

```{r}
#Block C individus
#parangons. per cadascun dels clusters tenim els seus parangons i els seus més espeicifis (dist) -> que es diferencien més dels altres clusters
res.hcpc$desc.ind
```

```{r}

#Donar-li una classe (the last one) a tots els outliers multidimensionals (sup.)
df$claHP<-7
df[row.names(res.hcpc$data.clust),"claHP"]<-res.hcpc$data.clust$clust
table(df$claHP)

#No ens hem de preocupar de la clase outliers, només caracteritizar els clusters del metode per defecte que genera la classificació jerarquica. 

```

#K-Means Classification

```{r}

ppcc<-res.pca$ind$coord[,1:6]
dim(ppcc)
kc<-kmeans(ppcc,6,iter.max = 30, trace=T)
table(kc$cluster)
```

```{r}

df$claKM<-7
dim(df$claKM)
df[names(kc$cluster),"claKM"]<-kc$cluster
kc$betweenss/kc$totss
```

```{r}
#caracteristació claKM
catdes(df, 44)
#veure si s'han posat d'acord o no
table(df$claHP,df$claKM)
df$claKM 
```

```{r}

df$claHP<-factor(df$claHP,labels=paste("kHP-",1:7))
df$claKM<-factor(df$claKM,levels=c(5,3,6,1,4,2,7),labels=c("kKM-5","kKM-3","kKM-6","kKM-1","kKM-4","kKM-2","kKM-7"))
tt<-table(df$claHP,df$claKM)
tt
sum(diag(tt)/sum(tt))

#assignar 
df[names(kc$cluster),"claKM"]<-factor(kc$cluster, levels=c(8,7,2,1,6,3,4,5), labels=c("kKM-8","kKM-7","kKM-2","kKM-1","kKM-6","kKM-3","kKM-4","kKM-5"))

#comparar
tt<-table(df$claHP,df$claKM)
sum(diag(tt)/sum(tt))

```


```{r, include=false}
kMeansCluster <- kmeans(res.pca$ind$coord, 4, nstart = 20)
kMeansCluster <- kmeans(res.pca$ind$coord[,1:6],center= 3)


kMeansCluster$clusterF <-as.factor(kMeansCluster$cluster)
elbowDF<-data.frame(res.pca$ind$coord)
ggplot(elbowDF, aes(elbowDF$Dim.1,elbowDF$Dim.2 , color = kMeansCluster$clusterF)) + geom_point()
```


