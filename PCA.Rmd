---
title: "PCA analysis of NYCABS datase"
author: "Katerina Dimitrova, Jose Romero, Sergi Munoz"
date: "March 18, 2018"
output: pdf_document
---

```{r,include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

```
```{r,include=FALSE}
setwd("C://Users/Sergi/Desktop/Sergi/ADEI") #Change 

```

#Previous work

## Load requiered packages

```{r,include=FALSE}
rm(list=ls())
requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","missMDA","mvoutlier","dplyr","ggmap","ggthemes","knitr","MVA")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

# Useful function
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3],        q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

```


#PCA analysis
1. The Kaiser rule is to drop all components with eigenvalues under 1.0 
   According to the Elbow rule when the drop ceases and the curve makes an elbow toward less steep declinewe should drop all further components after the one starting the elbow.
   
##I. I. Eigenvalues and  axes 
```{r}
load("Taxi5000_raw_libraryDay.RData")
library(FactoMineR)
names (df)
vars_con_pca<-names(df)[c(6,7,8,9,10,11,12,13,14,15,16,17,18,22,23,24,25,26)] 
res.pca<-PCA(df[,vars_con_pca], quanti.sup = 13, quali.sup = 14, ncp = 6 ) # TotalAmount and AnyTip
barplot(res.pca$eig[,1], main="Eigenvalues", names.arg = paste("dim", 1:nrow(res.pca$eig)))
plot(res.pca$eig[,1], type = "l") # line chart
length <-length(which(res.pca$eig[,1]>=1));length
kaiser <- res.pca$eig[1:length,1] #keep only EV >=1 ->first 6

#facto extra
fviz_eig(res.pca, addlabels = TRUE)
fviz_eig(res.pca, choice = "eigenvalue",addlabels = TRUE)
elbow <- kaiser ## we also stop at 6th dimension because of the graphic (the slope stops at 6th dimension)

```

##II.  Individuals point of view
## Look at variables that are too contributive
```{r}
summary(res.pca, dig = 2, nbelements = 17, nbind=3, ncp=4)

#ll1<-Boxplot(res.pca$ind$coord[,1]);ll1 #outliers from coord of all ind. for 1rst dimension
#rang1<-sort(res.pca$ind$coord[,1],decreasing = T)
#df[rang1[1:3], vars_con]

#graphic
plot.PCA(res.pca, choix=c("ind"),cex=0.8,col.ind="grey80",select="contrib15",axes=c(1,2))


#characteristic of extreme otliers in dim1 or sth like this
summary(res.pca$ind$coord[,1])
iqrvar<-IQR(res.pca$ind$coord[,1])
quantil3<-quantile(res.pca$ind$coord[,1], .75);quantil3 #get 3rd quartile
outliers<-which(res.pca$ind$coord[,1]>(iqrvar*3)+quantil3);length(outliers)


df$f.outlierPCAd1<-0
df[outliers,"f.outlierPCAd1"]<-1
df$f.outlierPCAd1<-factor(df$f.outlierPCAd1,labels=c("NoOutDim1", "YesOutDim1"))
summary(df$f.outlierPCAd1)
names(df)
#catdes(,names(df)[c(22)])


rang1<-sort(res.pca$ind$coord[,1],decreasing = T)
#rang1[1:3]
#df[rang1[1:3],vars_con]
df$epca1<-0
df$epca1[rang1[1:length(outliers)]]<-1

summary(df$epca1)
df$epca1<-factor(df$epca1,labels=c("NoOutDim1", "YesOutDim1"))
names(df)
catdes(df, 43)

```

##III Interpret axis
```{r}
# Interential criteria
dimdesc (res.pca, axes=1:4)

plot(res.pca,choix="var", cex = 0.75)
plot(res.pca,choix="var", cex = 0.75, axes = (3:4))# 3rd and 4th PCA
#modern factoextra

fviz_pca_var(res.pca,col.var="cos2", repel=TRUE)+scale_color_gradient2(low="green", mid="blue", high="red",midpoint=0.5)+theme_bw() #Almost

###last section missing
#lines(res.pca)#...


```

##IV PCA execution with supplementary individuals
```{r}
vec_out
#WHAT SHE HAS DONE IN CLASS (IT IS NOT OUR VECTORS SO IT WON'T WORK)
# f.hour it is not a good choice anyway, she prefers period (conclusions made in clusters analysis)
# we have to choose suplementary variable that would help us to interpret -> it's a good idea to use discretees homologous of the variables that contribute the most for pca projection.
res.pca<-PCA(df[,c(vars_con_pca, "f.fare", "f.tt", "f.hour")], ncp=4, quanti.sup=which(vars_con_pca=="Total_amount"), ind.sup = vec_out, quali.sup = 19:21, graph= FALSE)

```


#Hierarchical clustering

```{r}
library(FactoMineR)
library (ggplot2)
library(ggdendro)

summary(df[,vars_con])
#we need to do at least 6-7-8 clusters
res.hcpc <-HCPC(res.pca)
names(res.hcpc)
table (res.hcpc$data.clust$clust)
#Block A descripció per variables
res.hcpc$desc.var
#Block B descripció per eixos
#She doesn't recommend that (not very usefull by her opinion)
res.hcpc$desc.axes
#Block C individus
#parangons. per cadascun dels clusters tenim els seus parangons i els seus més espeicifis (dist) -> que es diferencien més dels altres clusters
res.hcpc$desc.ind

#Donar-li una classe (la última) a tots els outliers multidimensionals (sup.)

#No ens hem de preocupar de la clase outliers, només caracteritizar els clusters del metode per defecte que genera la classificació jerarquica. 

#clusters <- hclust(dist(elbowDF), method = 'mcquitty')
#ggdendrogram(clusters, rotate = FALSE, size = 2)
#plot(clusters)
#clusterCut <- cutree(clusters, 3)
#ggplot(elbowDF, aes(elbowDF$Dim.1,elbowDF$Dim.2)) + 
 # geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + A
  #scale_color_manual(values = c('yellow', 'red', 'green'))

```

#K-Means Classification

```{r}
#partitioning in k=7 groups
#res.pca<- PCA(df[,c(vars_con)], ncp=4,quanti.sup=which(vars_con=="Total_amount"),ind.sup = llvout, graph = FALSE)

ppcc<-res.pca$ind$coord[,1:4]
dim(ppcc)
kc<-kmeans(ppcc,6,iter.max = 30, trace=T)

df$claKM<-7
df[names(kc$cluster),"claKM"]<-kc$cluster
kc$betweenss/kc$totss

#caracterització claKM
names(df)
catdes(df, 44)
#veure si s'han posat d'acord o no
table(df$claHP, df$claKM)

#assignar 
df$claHp<-factor(df$claHP,labels=paste("kHp-",1:7))
df$claKM<-factor(df$claKM, levels=c(6,5,7,1,2,4,3,8), labels = ...)

#-----------------------------------------------#
kMeansCluster <- kmeans(res.pca$ind$coord, 4, nstart = 20)

kMeansCluster <- kmeans(res.pca$ind$coord[,1:6],center= 3)


kMeansCluster$clusterF <-as.factor(kMeansCluster$cluster)
elbowDF<-data.frame(res.pca$ind$coord)
ggplot(elbowDF, aes(elbowDF$Dim.1,elbowDF$Dim.2 , color = kMeansCluster$clusterF)) + geom_point()
```


# Multivariant Outliers Detection

```{r}
library(mvoutlier)
names(df)
vars_con # Problems with some variables
summary(df[,vars_con])
aq.plot(df[,vars_con]) # Problems when few numeric values are present in one variable

vars_con_out<-vars_con[c(1:4,6:7,14:16)]
mvout<-aq.plot(df[,vars_con_out])  # Problems when missing data are present
sum(mvout$outliers)

df$mvout<-factor(mvout$outliers)
catdes(df,which(names(df)=="mvout"))
```

# Repeat PCA using multivariant outliers as supplementary observations

```{r}
par(mfrow=c(1,1))
llvout<-which(df$mvout==TRUE)
#llout<-unique(c(llout,llvout))
res.pca<-PCA(df[,c(vars_conpca)],ncp=4,quanti.sup=which(vars_conpca=="Total_amount"),ind.sup=llvout)
